{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A lecture on making code **fast**\n",
    "(Designed for quantitative macro and similar structural econ in Python, but relevant for everyone!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First meta-principle: \"premature optimization is the root of all evil\"\n",
    "From TeX inventor and computer scientist extraordinaire Donald Knuth:\n",
    "> Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: **premature optimization is the root of all evil**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First meta-principle: \"premature optimization is the root of all evil\"\n",
    "Actual full quote:\n",
    "> Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. **Yet we should not pass up our opportunities in that critical 3%.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Second meta-principle: profile, profile, profile\n",
    "\n",
    "- How do you avoid premature optimization and identify that \"critical 3%\"?\n",
    "- **Profiling** your code!\n",
    "- Profiling tells you how much time (or memory) each part of your code takes to run\n",
    "- Your tool *before* thinking about optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What profiling looks like (line profiling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After typing `%load_ext line_profiler` and `%lprun -f sim.backward_iteration sim.steady_state(**sim.example_calibration())` after importing the `sim_steady_state_fast.py` module at https://github.com/shade-econ/nber-workshop-2023/blob/main/Lectures/sim_steady_state_fast.py as `sim` , we get the results of *line profiling* the backward iteration function in our code, as it is applied to solve for the steady state. (I'm eliminating the comment lines for brevity.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Timer unit: 1e-09 s\n",
    "    \n",
    "    Total time: 0.021981 s\n",
    "    File: /Users/matthewrognlie/Dropbox/joint folders/HANK minicourse/NBER workshop 2023/Lectures/sim_steady_state_fast.py\n",
    "    Function: backward_iteration at line 89\n",
    "    \n",
    "    Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "    ==============================================================\n",
    "\n",
    "    89                                               def backward_iteration(Va, Pi, a_grid, y, r, beta, eis):\n",
    "    91          542    3848000.0   7099.6     17.5      Wa = beta * Pi @ Va\n",
    "    94          542     981000.0   1810.0      4.5      c_endog = Wa**(-eis)\n",
    "    95          542    3420000.0   6310.0     15.6      coh = y[:, np.newaxis] + (1+r)*a_grid\n",
    "    97          542    9977000.0  18407.7     45.4      a = interpolate_monotonic_loop(coh, c_endog + a_grid, a_grid)\n",
    "    101         542     363000.0    669.7      1.7      setmin(a, a_grid[0])\n",
    "    102         542    1083000.0   1998.2      4.9      c = coh - a\n",
    "    105         542    2208000.0   4073.8     10.0      Va = (1+r) * c**(-1/eis)\n",
    "    107         542     101000.0    186.3      0.5      return Va, a, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What profiling looks like, continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typing `%prun sim.steady_state(**calib)` in the same notebook, having stored the calibration in `calib`, we get the results of Python's standard profiling of the steady-state function. `tottime` is the amount of time in that function specifically (not including other functions that it calls), while `cumtime` is the amount of time in the function total (including functions that it calls)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4046 function calls in 0.038 seconds\n",
    "\n",
    "    Ordered by: internal time\n",
    "    \n",
    "    ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "        542   0.018    0.000    0.028    0.000 sim_steady_state_fast.py:89(backward_iteration)\n",
    "        542   0.010    0.000    0.010    0.000 sim_steady_state_fast.py:168(interpolate_monotonic_loop)\n",
    "        581   0.005    0.000    0.005    0.000 sim_steady_state.py:137(forward_policy)\n",
    "        581   0.003    0.000    0.009    0.000 sim_steady_state.py:151(forward_iteration)\n",
    "        1     0.001    0.001    0.029    0.029 sim_steady_state_fast.py:110(policy_ss)\n",
    "        1128  0.000    0.000    0.000    0.000 serialize.py:30(_numba_unpickle)\n",
    "        1     0.000    0.000    0.009    0.009 sim_steady_state_fast.py:237(distribution_ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Very simple profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `%time` to figure out how long evaluating a single line takes (`%%time` if we want to time an entire Jupyter cell).\n",
    "\n",
    "Subtlety: \"Wall time\" is literally how much time elapsed while running the command, \"CPU times\" is how much total time CPUs spent on the command specifically (wall time might be higher if computer did other stuff while running, CPU time might be higher if used multiple cores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 269 µs, sys: 15 µs, total: 284 µs\n",
      "Wall time: 414 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49995000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sum(i for i in range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 519 µs, sys: 5 µs, total: 524 µs\n",
      "Wall time: 584 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x = 0\n",
    "for i in range(10000):\n",
    "    x += i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Alternative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use `%timeit` rather than `%time`, the line is run many times rather than one, and various other measures are taken to get a more accurate, usually lower, measure of how long the code takes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231 µs ± 1.47 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sum(i for i in range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 µs ± 561 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "x = 0\n",
    "for i in range(10000):\n",
    "    x += i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summing up profiling\n",
    "- Today we'll mainly use `%timeit` to profile simple fragments of code\n",
    "- On more complicated projects, we need things like `%lprun` (line profiling) and `%prun` (profiling how long each function takes) to get a handle on things\n",
    "- Commands starting with `%` are Jupyter/IPython \"magic\" commands, not Python code\n",
    "    * but all these profiling tools can be invoked from regular Python code if you import the right module (e.g. the `timeit` module)\n",
    "- Every language has profiling tools (though some are better or easier to use than others)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Caveat before we get started\n",
    "- Won't talk here about **parallelization** or **GPUs**, even though both can be important for the most intensive applications\n",
    "\n",
    "- But many lessons here will carry over...\n",
    "    * Profiling, keeping track of big-O complexity, reducing to efficient operations like matrix multiplication that are easily parallelized and put on GPUs...\n",
    "\n",
    "- Computation times today will be from running notebook on my Macbook Air M2 laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Now on to the main content..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Six principles\n",
    "1. Compiled is quick, the interpreter inches\n",
    "2. Memory matters\n",
    "3. Big O is a big deal\n",
    "4. Favor fast functions\n",
    "5. Savor sparsity and structure\n",
    "6. Don't duplicate drudgery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Compiled is quick, the interpreter inches\n",
    "- Some languages (e.g. C, C++, Fortran) require *compilation* to machine code before running, which is then executed directly by the processor (fast)\n",
    "- Other languages (e.g. Python, R, Matlab) are *interpreted*: an \"interpreter\" pretty much directly runs your commands\n",
    "    - Takes a lot longer to run, because the interpreter has to do a lot of housekeeping (e.g. look up where a variable is stored, what type it is, what a command means for that type)\n",
    "    - But it's more interactive and easier to get started, since there's no annoying compilation step\n",
    "    - Also makes it easy to avoid doing housekeeping yourself like declaring variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: add two random arrays, loop vs. direct\n",
    "300x-400x speed difference?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.random.rand(1000)\n",
    "y = np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148 µs ± 4.85 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "z = np.empty(1000)\n",
    "for i in range(1000):\n",
    "    z[i] = x[i] + y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "433 ns ± 10.3 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit z = x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's going on?\n",
    "- When we use a pure Python construct like a loop, it uses the interpreter\n",
    "- Other things are function calls passed through interpreter too\n",
    "    - Look up element `i` in an array, add two numbers, assign element `i` in an array\n",
    "- When we write `z = x + y`, the NumPy package calls compiled code under the hood to do the addition fast\n",
    "- These \"vectorized\" commands avoid spending much time in the interpreter, because we're giving high-level instructions to compiled code\n",
    "- Most of you probably think of this as rule 1: vectorized good, loops bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Do loops need to be bad?\n",
    "An amazing package called **Numba** allows us to do \"just-in-time\" compilation of many Python functions (with NumPy numerical code) just by applying a \"decorator\" to the function.\n",
    "\n",
    "Let's apply this to the inefficient loop code we wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "@numba.njit\n",
    "def add(x, y):\n",
    "    z = np.empty(1000)\n",
    "    for i in range(1000):\n",
    "        z[i] = x[i] + y[i]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Numba results\n",
    "The first time we run it, it's slow, because Numba has to compile the function and that takes time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 188 ms, sys: 34.4 ms, total: 223 ms\n",
      "Wall time: 300 ms\n"
     ]
    }
   ],
   "source": [
    "%time z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But after that, it has a speed not far from directly writing `z = x + y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636 ns ± 16.3 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit z = add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How slow is interpreted code in general?\n",
    "We saw almost a 400x speed difference in one case. It's useful to experiment with different examples to get a sense of relative cost.\n",
    "\n",
    "Let's compare code that adds these 1000-element vectors to code that retrieves items from a dictionary that maps 1000 numbers to their squares. This is more like a 60x difference: here, addition of NumPy vectors takes slightly less than 0.5 ns per element, while loop with integer dictionary accesses takes just above 25 ns per element (still quite fast)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444 ns ± 4.36 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit z = x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_to_squares = {i: i**2 for i in range(1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.1 µs ± 201 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for i in range(1000):\n",
    "    square = numbers_to_squares[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Be careful: use NumPy functions to operate on arrays\n",
    "- Common way to trip up beginners!\n",
    "- There's a standard Python function called `sum` that can do a sum over anything that's iterable using a for loop - basically, it runs a for loop for you.\n",
    "- But this spends a lot of time in the interpreter, so is slow, and you want the NumPy function `np.sum`: another case of the interpreter inching along, and compiled being quick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.3 µs ± 210 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.44 µs ± 23 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Memory matters\n",
    "- Here's where we start to get into things you *don't* know!\n",
    "- It turns out that reads and writes to *memory*, not processing by the CPU itself, is often the speed bottleneck\n",
    "- Huge speed differences in code possible when we avoid needless reads and writes, and keep as much as possible in the processor's *cache*\n",
    "- A lot of people who think, say, that Fortran is inevitably way faster than Python, don't understand this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What to do when memory matters? Avoid temporaries!\n",
    "Suppose we want to evaluate $z = f(x,y) \\equiv x^2 + 3xy + y^2 - 2x - 4y +1$.\n",
    "\n",
    "Naive (though easy-to-read) option is to just write it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.53 µs ± 100 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit z = x**2 + 3*x*y + y**2 - 2*x - 4*y + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From Python's perspective, what's happening is similar to...\n",
    "Writing and reading a ton of intermediate arrays: a lot of wasteful dealing with memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.64 µs ± 134 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "z1 = x**2\n",
    "z2 = x*y\n",
    "z3 = 3*z2\n",
    "z4 = y**2\n",
    "z5 = 2*x\n",
    "z6 = 4*y\n",
    "z7 = 1\n",
    "z = z1 + z3 + z4 - z5 - z6 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Compiled with Numba, a loop would be much faster\n",
    "Because they directly do the computation with $x$ and $y$ one entry at a time, with everything probably staying in the processor's registers, and run through the arrays in order without creating intermediates (which helps computer efficiently load memory into cache one block at a time), compiled loops are actually *much better than vectorization* - 7 times faster in this case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def f(x, y):\n",
    "    z = np.empty_like(x)\n",
    "    for i in range(len(x)):\n",
    "        z[i] = x[i]**2 + 3*x[i]*y[i] + y[i]**2 - 2*x[i] - 4*y[i] + 1\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899 ns ± 14.9 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "f(x, y) # run once to compile\n",
    "%timeit f(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# That's right, (compiled) loops can be *better* than vectorization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But sometimes loops are uglier...\n",
    "- One problem with this strategy is that it's often very nice to be able to just write an expression directly in vectorized form\n",
    "- Loops add unnecessary housekeeping and make the code less clean\n",
    "- Certainly don't do this if it's not really important to speed up your code (avoid premature optimization)\n",
    "    * Also, tools like JAX work are designed for vectorized rather than loop-based code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cleaner way\n",
    "Fortunately, Numba provides a cleaner way to do this, the \"vectorize\" decorator, which turns a function on scalars into a compiled function that will evaluate element-by-element on vectors (or larger arrays).\n",
    "\n",
    "Instead of making your code less clean, this can actually make your code *more* clean - since it's good practice to put fairly-complex calculations into their own functions and give them descriptive names (more descriptive than `f`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.vectorize\n",
    "def f(x, y):\n",
    "    return x**2 + 3*x*y + y**2 - 2*x - 4*y + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1 µs ± 14.6 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "f(x, y) # run once to compile\n",
    "%timeit f(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Caveat: other languages\n",
    "- This gives a huge improvement in Python because in Python, all those unnecessary temporary arrays are always created, at great cost, unless you specify otherwise\n",
    "- Matlab sometimes is smart and avoids this\n",
    "    - but not always, and you never know when\n",
    "- If the compiler is smart enough in a compiled language with arrays (e.g. Fortran), it will avoid this\n",
    "    - but again very dependent on the language, compiler, etc., not super predictable\n",
    "    - get the most predictable performance by writing loops\n",
    "- In Julia, you can avoid this by using its \"dot\" syntax (basically its analog of `numba.vectorize`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# More memory matters: taking dot product\n",
    "Often need to do this when aggregating individual variables across some distribution.\n",
    "\n",
    "First version creates a temporary `x*y` array and then sums: much slower!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.89 µs ± 2.82 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.sum(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "808 ns ± 5.66 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.vdot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Even more memory mattering...\n",
    "Two identical computations, but wasteful to create a vector filled with ones and then have it be accessed to do addition, rather than just putting a 1 there and letting NumPy handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.65 µs ± 8.97 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.ones(1000) + x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24 µs ± 4.89 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit 1 + x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Big O is a big deal\n",
    "- Now we're onto some (very basic) computer science theory\n",
    "- Suppose the size of a problem is in some way described by $N$ (e.g. we're operating on an array with $N$ entries)\n",
    "- If we say that the time for an algorithm is $O(f(N))$, that means that for large enough $N$, the cost of the algorithm is bounded from above by $M\\cdot f(N)$, where $M$ is some constant\n",
    "- Good for getting a handle on cost: $O(\\log N)$ is log, $O(N)$ is linear, $O(N \\log N)$ is log times linear, $O(N^2)$ is quadratic, etc.\n",
    "- For simple algorithms can get it by counting operations (or otherwise informally)\n",
    "- Called **Big O notation** if you want to read further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem: locating elements in array\n",
    "- Problem: for each element of `X`, find which elements of `Y` it's between\n",
    "- Suppose `X` and `Y` are sorted vectors, and we want to return a vector that says for each element `X[i]`, the index `j` such that `X[i]` is between `Y[j]` and `Y[j+1]`\n",
    "- If `X[i]` is lower than all elements of `Y`, put -1, and if it's greater than all elemnts of `Y`, put -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple $O(N^2)$ code to handle this\n",
    "Loop through: for each element `X[i]`, go through all the elements `Y[j]` until we find first one that's bigger, then we know it's between `Y[j-1]` and `Y[j]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def find_indices(X, Y):\n",
    "    js = np.full(len(X), -2) # default -2 if we never find it\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(Y)):\n",
    "            if X[i] < Y[j]:\n",
    "                js[i] = j-1\n",
    "                break\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  2, -2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_indices(np.array([1, 2, 2.8, 3]),\n",
    "             np.array([1.5, 2.5, 2.7, 2.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can we see the $O(N^2)$?\n",
    "Supposing that `X` and `Y` are both length $N$, this will be $O(N^2)$, since we have to loop through all elements of $X$ (that's one factor of $N$), and then on average if we have to loop through half the elements of `Y` to find one we have $N/2$ (another factor of $N$ when we drop constants).\n",
    "\n",
    "When we increase the size of the problem by a factor of 10, time taken grows by a factor of 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153 µs ± 2.06 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "X = np.sort(np.random.rand(1000))\n",
    "Y = np.sort(np.random.rand(1000))\n",
    "%timeit find_indices(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.9 ms ± 190 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "X = np.sort(np.random.rand(10000))\n",
    "Y = np.sort(np.random.rand(10000))\n",
    "%timeit find_indices(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Alternative algorithm: $O(N \\log N)$ binary search\n",
    "Since we know `Y` is sorted, there's a better way to search than just running linearly through all elements of `Y`. We can do a *divide and conquer* approach, called a binary search (bisection is the continuous version). Let's code this up to find any `x` in `Y`, then write as a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def binary_search(x, Y):\n",
    "    if x < Y[0]:\n",
    "        return -1\n",
    "    elif x > Y[-1]:\n",
    "        return -2\n",
    "    else:\n",
    "        low = 0\n",
    "        high = len(Y) - 1\n",
    "        while high - low > 1:\n",
    "            mid = (low + high) // 2 # // operator is \"division with round down to nearest integer\"\n",
    "            if x < Y[mid]:\n",
    "                high = mid\n",
    "            else:\n",
    "                low = mid\n",
    "        return low\n",
    "    \n",
    "@numba.njit\n",
    "def find_indices_binary(X, Y):\n",
    "    js = np.empty(len(Y), dtype=np.int64)\n",
    "    for i in range(len(X)):\n",
    "        js[i] = binary_search(X[i], Y)\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  2, -2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_indices_binary(np.array([1, 2, 2.8, 3]),\n",
    "                    np.array([1.5, 2.5, 2.7, 2.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can we see the $O(N \\log N)$?\n",
    "It turns out that each application of the binary search takes $O(\\log N)$, because we halve the size of the interval each time, so the number of steps is approximately $\\log_2(N)$, and then we drop the constant for big O. So doing this for every element of `X` is $O(N \\log N)$.\n",
    "\n",
    "Sure enough, we increase at a rate slightly worse than linear when we raise the size of the problem by 10. Also, the first case is almost 30 times faster than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485 µs ± 5.33 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "X = np.sort(np.random.rand(10000))\n",
    "Y = np.sort(np.random.rand(10000))\n",
    "%timeit find_indices_binary(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.81 ms ± 58.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "X = np.sort(np.random.rand(100000))\n",
    "Y = np.sort(np.random.rand(100000))\n",
    "%timeit find_indices_binary(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Best algorithm: $O(N)$ linear search exploiting `X` sorted\n",
    "The previous algorithm used the fact that `Y` was sorted to do a binary search (divide and conquer) and speed things up from $O(N^2)$ to $O(N \\log N)$. No assumptions were needed for `X`.\n",
    "\n",
    "But we actually assumed that `X` was sorted too. That means we don't have to start from the beginning of `Y` each time: we know that the index of `X[i+1]` will be higher than that of `X[i]`.\n",
    "\n",
    "Just need to make *slight* modification to our first function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def find_indices_smart(X, Y):\n",
    "    js = np.full(len(X), -2) # default -2 if we never find it\n",
    "    j = 0\n",
    "    for i in range(len(X)):\n",
    "        for j in range(j, len(Y)): # start from where you ended up last time!\n",
    "            if X[i] < Y[j]:\n",
    "                js[i] = j\n",
    "                break\n",
    "    return js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  3, -2])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_indices_smart(np.array([1, 2, 2.8, 3]),\n",
    "                   np.array([1.5, 2.5, 2.7, 2.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Can we see the $O(N)$?\n",
    "Once we've determined that `X[i]` is between `Y[3]` and `Y[4]`, we start our search for `X[i+1]` between `Y[3]` and `Y[4]`, skipping earlier entries. This strategy avoids redundantly going through large parts of `Y` and turns out to actually have *linear* time $O(N)$.\n",
    "\n",
    "The middle case is about 7.5 times faster than the comparable case for our $O(N \\log N)$ algorithm, probably partly because it has a more regular memory access pattern too. (Note: growth still a bit more than linear in $N$ in practice, probably due to memory issues.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.5 µs ± 1.64 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "X = np.sort(np.random.rand(10000))\n",
    "Y = np.sort(np.random.rand(10000))\n",
    "%timeit find_indices_smart(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887 µs ± 7.98 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "X = np.sort(np.random.rand(100000))\n",
    "Y = np.sort(np.random.rand(100000))\n",
    "%timeit find_indices_smart(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.58 ms ± 142 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "X = np.sort(np.random.rand(1000000))\n",
    "Y = np.sort(np.random.rand(1000000))\n",
    "%timeit find_indices_smart(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bottom line\n",
    "- Different algorithms to solve the same problem make a huge difference\n",
    "- By learning to figure out the big-O complexity of an algorithm, you can quickly gauge\n",
    "    - whether it might underperform on larger problems\n",
    "    - whether it might be possible to improve (can we reduce $O(N^2)$ to $O(N \\log N)$ to $O(N)$ like here?)\n",
    "    \n",
    "    \n",
    "- Applies to all discrete operations\n",
    "    - for continuous things where we need to converge to a solution within some tolerance (e.g. root finding), there are related other metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Favor fast functions\n",
    "- Some functions are much faster than others, and knowing which can save a lot of time\n",
    "    - under the hood, this might be for \"memory matters\" or \"big O is a big deal\" reasons\n",
    "\n",
    "    \n",
    "- For simple, core functions like sine or matrix multiplication, the built-in implementation will be usually much faster than what you can do\n",
    "    - with a few notable exceptions...\n",
    "    \n",
    "    \n",
    "- For more complex functions, it really depends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: matrix multiplication\n",
    "We could code this \"by hand\" with loops in Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def multiply_by_hand(X, Y):\n",
    "    Z = np.zeros((X.shape[0], Y.shape[1]))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            for k in range(X.shape[1]):\n",
    "                Z[i, j] += X[i, k] * Y[k, j]\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to make sure we remembered matrix multiplication correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(200, 200)\n",
    "Y = np.random.rand(200, 200)\n",
    "np.allclose(X @ Y, multiply_by_hand(X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# But we are a LOT slower than standard matrix multiplication\n",
    "By a factor of over 20!\n",
    "\n",
    "It turns out that matrix multiplication is *so* important that it's very, very efficiently implemented in most numerical libraries.\n",
    "\n",
    "One reason for the greater speed on my machine is that it knows how to use multiple cores efficiently (which we didn't enable in our Numba code).\n",
    "\n",
    "A million other subtle, careful things that experts know how to get right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271 µs ± 9.64 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit X @ Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.75 ms ± 50.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit multiply_by_hand(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example going the other way: powers\n",
    "Turns out that powers (other than special cases 0, 1, -1, and 2) are extremely slow, especially in the implementation used by NumPy, compared to basic arithmetic operations like multiplication, by a factor of 10 or more. If everything else is optimized, this often ends up being a bottleneck in your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(1000) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615 ns ± 12.2 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit 2.3*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.15 µs ± 45.7 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit x**2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So much that we can actually speed things up a bit if we actually do the power ourselves with exp and log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.04 µs ± 7.1 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.exp(2.3*np.log(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Verify that the error is tiny:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relerr = np.exp(2.3*np.log(x)) / x**2.3 - 1\n",
    "np.max(np.abs(relerr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is [machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon), the minimum relative difference between two numbers in the computer's double-precision [floating point arithmetic](https://en.wikipedia.org/wiki/Floating-point_arithmetic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.220446049250313e-16"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is pretty much as accurate as we can possibly hope, short of exact accuracy up to machine precision.\n",
    "\n",
    "Why is the computer's power function actually slower than this (and why doesn't it just use the `exp` `log` trick instead)? It's hard to say: could be obscure implementation details, like spending some time optimizing special cases. Or it could be the fact that it's costly to make sure that the number is *exactly* right up to rounding to machine precision, which can be a lot harder than having a machine epsilon error like we did above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Polynomial evaluation\n",
    "Suppose we want to evaluate a polynomial like $6x^4 + 3x^3 + 2x^2 - 4x + 4$ for all points in a large vector `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(10000)\n",
    "p = np.array([6, 3, 2, -4, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: direct evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 µs ± 1.71 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit p[0]*x**4 + p[1]*x**3 + p[2]*x**2 + p[3]*x + p[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: NumPy's function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.4 µs ± 2.32 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.polyval(p, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is NumPy doing 4 to 5 times better than ours? Well, we already have two ideas:\n",
    "- First, non-special-case powers are slow, so `x**4` and `x**3` are inefficient.\n",
    "- Second, for \"memory matters\" reasons, writing out vectorized expressions explicitly like this is slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Faster polynomial evaluation\n",
    "Method 3: use multiplication rather than powers. This already gets almost as fast as NumPy, a 5-fold improvement on the original!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 µs ± 3.24 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit p[0]*x*x*x*x + p[1]*x*x*x + p[2]*x*x + p[3]*x + p[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 4: [Horner's method](https://en.wikipedia.org/wiki/Horner%27s_method), the standard efficient way to evaluate polynomials. This rearranges the expression above in a smart way to reduce the number of multiplication signs `*` from 10 to 4, while keeping the number of addition signs `+` the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5 µs ± 536 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit p[4] + x*(p[3] + x*(p[2] + x*(p[1] + p[0]*x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that this is what NumPy's `polyval` function uses, but by avoiding the overhead of the function we're already doing better than it (and almost 10 times better than our original expression)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fasterrrrrrrrrrrr!!!!!!!!\n",
    "We know from \"memory matters\" that it's inefficient to have so many temporary arrays created, so let's use `numba.vectorize`. This speeds us up by a factor of almost 10, so that we are 50 times faster than our original code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.vectorize\n",
    "def f_horner(x):\n",
    "    return p[4] + x*(p[3] + x*(p[2] + x*(p[1] + p[0]*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.83 µs ± 107 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "f_horner(x)\n",
    "%timeit f_horner(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# You would have thought that writing the polynomial out directly, in vectorized code, was a reasonable approach... but here we just did 50 times better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bottom line\n",
    "- This was a bit of a grab-bag...\n",
    "\n",
    "\n",
    "- But the main idea is that there's quite a bit of variation in the speeds that different functions take, even when you think it's obvious or that it can't be that different:\n",
    "    - NumPy did matrix multiplication 20 times faster than us...\n",
    "    - but with Numba we evaluated a polynomial 10 times faster than NumPy's `polyval`!\n",
    "    - powers are really slow, though we can make them a bit faster\n",
    "    \n",
    "    \n",
    "- A few simple rules can get you most of the way\n",
    "    - most elementary functions and also key mathematical operations like matrix multiplication and the FFT are implemented super-fast\n",
    "    - other stuff more questionable\n",
    "    \n",
    "- But there's no substitute for using a lot of `%timeit` if you want to become a guru of these things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Savor sparsity and structure\n",
    "I just told you that matrix multiplication is really, really fast.\n",
    "\n",
    "But one foolish thing a lot of people do is to transcribe their matrix notation directly into code, without thinking about what's going on.\n",
    "\n",
    "If you don't take advantage of *sparsity* and *structure*, you can end up with extremely costly large matrix operations - since multiplying or inverting $N$-by-$N$ matrices is cubic: $O(N^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Independent laws of motion\n",
    "It's fairly common to have two states that obey independent laws of motion. We often simplify notation by combining these two states into one state, and combining the Markov transition matrices using the Kronecker product.\n",
    "\n",
    "Suppose these states can each have 50 values apiece. Let `D` by a 50-by-50 distribution across two states, and let them have independent laws of motion `Pi1` and `Pi2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.random.rand(50,50)\n",
    "Pi1 = np.random.rand(50,50)\n",
    "Pi2 = np.random.rand(50,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Kronecker product to get the combined 2500-by-2500 transition matrix `Pi`. (Note that this is *huge*, with $2500^2 / (2\\cdot 50^2) = 1250$ times more entries than `Pi1` and `Pi2` combined.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi = np.kron(Pi1, Pi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's faster, independent updates or the Kronecker product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kronecker product approach: apply the Markov transition matrix `Pi` (on the right) to the flattened distribution (we use `D.ravel()` to flatten - unlike `D.flatten()`, it doesn't make a copy):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.29 ms ± 344 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit D.ravel() @ Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other approach: apply the Markov transition matrices `Pi1` and `Pi2` separately to the two-dimensional array `D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.4 µs ± 210 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit Pi1.T @ D @ Pi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference in speed is over a factor of 100! What's responsible for this? In principle, the first approach is one 2500-by-2500 times 2500 matrix-vector multiplication, complexity $2500^2$, vs. the second approach which is two 50-by-50 times 50-by-50 matrix-matrix multiplications, complexity $2\\times 50^3$.\n",
    "\n",
    "The ratio of these two is only 25 - how did we get an even bigger difference in speed? Probably because *memory matters*: much more can stay in the cache and be accessed quickly if we're only dealing with a few 50-by-50 matrices than if we're dealing with a 2500-by-2500 matrix.\n",
    "\n",
    "Finally, verify that the results are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(D.ravel() @ Pi, (Pi1.T @ D @ Pi2).ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Structure: exploit it!\n",
    "Moral of the story: turns out that writing everything in standard matrix-vector notation, even if there's a more concise way to represent objects, is a really bad idea in computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Trick to help out: einsum\n",
    "One trick for keeping track of coordinates in messy multidimensional problems is to use the brilliant `np.einsum` command - a bit slower than matrix multiplication (sometimes much slower if it's not \"optimized\") but extremely transparent once you get the hang of it, infinitely better than the traditional Matlab cavalcade of transposes and swapping axes and Kronecker products.\n",
    "\n",
    "The following says that we start with a distribution with coordinates $i,j$, and `Pi1` takes coordinate $i$ to $k$ while `Pi2` takes coordinate $j$ to $l$, to get a new distribution with coordinates $k,l$. (In this case, the `->kl` actually isn't necessary, but I have it there to be explicit. This \"Einstein\" notation just says to take a sum over dimensions $i$ and $j$ of all terms $D_{i,j}\\Pi^1_{i,k}\\Pi^2_{j,l}$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.einsum('ij,ik,jl->kl', D, Pi1, Pi2)\n",
    "np.allclose(result, Pi1.T @ D @ Pi2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Side note**: This is actually a (rare) case where the default `einsum` is much slower. We need to feed `optimize=True` for it to go fast (or precalculate the optimal path using `einsum_path` and write it into the code), but that fixed effort of doing so takes too many microseconds for ultra-tiny problems like this one.\n",
    "\n",
    "For larger examples, there can still be a performance hit relative to matrix multiplication, but not always:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = np.random.rand(500,500)\n",
    "Pi1 = np.random.rand(500,500)\n",
    "Pi2 = np.random.rand(500,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.71 ms ± 418 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit Pi1.T @ D @ Pi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.19 ms ± 78.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit np.einsum('ij,ik,jl->kl', D, Pi1, Pi2, optimize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sparse transition rules\n",
    "Another very common occurence is that we have *sparse* Markov transition matrices that describe movements between states.\n",
    "\n",
    "For instance, suppose that we have 1000 states (perhaps these are gridpoints for assets), and the agent in state $i$ has a policy of going to state $i+1$ with probability $1/(i+1)$, and staying at state $i$ with probability $i/(i+1)$ - except at the top state, where she stays with probability 1.\n",
    "\n",
    "How do we apply this Markov matrix to update a distribution? A **very bad** approach is to explicitly construct the Markov matrix `Pi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pi = np.diag(np.arange(1000) / (np.arange(1000)+1)) + np.diag(1 / (np.arange(999)+1), 1)\n",
    "Pi[-1, -1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376 µs ± 50.6 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "D = np.random.rand(1000)\n",
    "%timeit D @ Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built-in matrix multiplication is so fast that this isn't *horrible*, but still it's incredibly inefficient: we're using a 1000-by-1000 matrix even though almost all its entries are zeroes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Better approaches\n",
    "One less-terrible approach, which is very popular, is to still explicitly construct `Pi`, but do so with sparse matrix libraries. This has some advantages - we get to stick to matrix notation - but it can still be annoying to deal with sparse matrix libraries, and often slower because we can't specialize to our specific case.\n",
    "\n",
    "The best approach is simply to *use loops* - compiled with Numba, of course - to directly implement this. Often we can be a bit clever in writing the function to minimize memory accesses or computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def update(D):\n",
    "    Dnew = D.copy()\n",
    "    for i in range(len(D)-1):\n",
    "        movers = D[i] / (i+1)\n",
    "        Dnew[i] -= movers\n",
    "        Dnew[i+1] += movers\n",
    "    return Dnew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: almost 400 times faster than the naive matrix multiplication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.04 µs ± 0.827 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "update(D)\n",
    "%timeit update(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Real life: these issues are often combined\n",
    "- It's not an accident that I gave these examples!\n",
    "- In het-agent models, it's quite common that the Markov matrix combines several independent processes (or at least smaller Markov matrices that can be applied sequentially), and some of these are sparse.\n",
    "- Extreme efficiency or inefficiency possible depending on whether we adopt the lessons here:\n",
    "    - Apply separate Markov matrices rather than using Kronecker products\n",
    "    - Directly write code to handle sparse transitions rather than building matrices\n",
    "- Of course, these lessons are more important when the number of states is large and the $O(N^3)$ of ordinary matrix multiplication really bites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Don't duplicate drudgery\n",
    "\n",
    "- Often the same calculation is needed in different places in your code\n",
    "\n",
    "\n",
    "- Sounds obvious, but you don't want to repeat this calculation if at all possible!\n",
    "    * Harder than it sounds!\n",
    "    \n",
    "    \n",
    "- Figuring out what expensive things you can \"precompute\" and then reuse many, many times is an important trick\n",
    "    * Tricky real-life examples will have to wait until we're looking at more complex code, though"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Calculations in the right order\n",
    "It's fairly common to need to multiply an array by a bunch of constants.\n",
    "\n",
    "You should multiply all the constants together *first* and then multiply by the array, not vice versa, so that there's only one scalar-array multiplication, rather than a bunch of temporary arrays created. (This is also a \"memory matters\" issue.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2.3\n",
    "b = 6.1\n",
    "c = 3.7\n",
    "d = -0.3\n",
    "x = np.random.rand(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slow (calculates array `x*a`, then multiplies this by `b` to make a new array, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.45 µs ± 3.23 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit x*a*b*c*d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fast (multiplies all the scalars together first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "655 ns ± 3.81 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit (a*b*c*d)*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually don't need the parentheses, since evaluation is left-to-right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657 ns ± 3.69 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit a*b*c*d*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Think if you've already solved a nearby problem\n",
    "Suppose we're trying to calculate the $N$ complex roots of unity $e^{2 i \\pi n/N}$ for $n=0,\\ldots,N-1$, which go around the unit circle in the complex plane.\n",
    "\n",
    "Naive approach (but at least multiplying scalars first, since we just learned that):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 µs ± 635 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "%timeit np.exp(2j*np.pi/N*np.arange(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smarter approach: consecutive entries only differ by constant factor `np.exp(2j*np.pi/N)`, so we can calculate this *once* and then recursively build up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def smart_unity(N):\n",
    "    roots = np.empty(N, dtype=np.complex128)\n",
    "    roots[0] = 1\n",
    "    factor = np.exp(2j*np.pi/N)\n",
    "    for i in range(1, N):\n",
    "        roots[i] = factor*roots[i-1]\n",
    "    return roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 µs ± 20.1 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "smart_unity(N)\n",
    "%timeit smart_unity(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saved a factor of 10! (And we could save some more if we realized that after we calculate half, the other next half is just the complex conjugates.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note: interestingly, we could get these from the Fast Fourier Transform, which is very efficiently implemented on the computer, but because this is $O(N log N)$ rather than $O(N)$, it's actually a bit slower here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.9 µs ± 2.6 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "assert np.allclose(np.fft.fft(np.arange(N)==(N-1)), smart_unity(N))\n",
    "%timeit np.fft.fft(np.arange(N)==(N-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Don't unintentionally call the same function many times\n",
    "Sometimes you call the same function many times because it's difficult to keep track of the hierarchy of functions, and where it might be called.\n",
    "\n",
    "As an extreme (maybe contrived) example, imagine that we want to calculate the Fibonacci sequence and decide to do the recursive algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def fib(n):\n",
    "    if n < 2:\n",
    "        # base cases fib(0)=0, fib(1)=1\n",
    "        return n\n",
    "    else:\n",
    "        return fib(n-1) + fib(n-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.6 µs ± 18.8 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "3.4 ms ± 20.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "fib(20)\n",
    "%timeit fib(20)\n",
    "%timeit fib(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost explodes exponentially because when we calculate `fib(30)`, we're for instance calling `fib(28)` twice (once directly and once for `fib(29)`), and `fib(5)` a zillion times.\n",
    "\n",
    "If we write it iteratively instead, we do far better, because we only calculate each value once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def fib_iterative(n):\n",
    "    if n < 2:\n",
    "        return n\n",
    "    \n",
    "    fs = np.empty(n+1)\n",
    "    fs[0] = 0\n",
    "    fs[1] = 1\n",
    "    \n",
    "    for i in range(2, n+1):\n",
    "        fs[i] = fs[i-1] + fs[i-2]\n",
    "   \n",
    "    return fs[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 ns ± 1.39 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)\n",
      "217 ns ± 1.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      "36.8 µs ± 858 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "fib_iterative(20)\n",
    "%timeit fib_iterative(20)\n",
    "%timeit fib_iterative(30)\n",
    "%timeit fib_iterative(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Refresher: memory matters, let's avoid reads and writes\n",
    "Instead of previous code, which built up Fibonacci numbers in an array (which would, of course, be useful if we wanted that array for later), write the following that avoids memory reads and writes alogether:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.njit\n",
    "def fib_nomemory(n):\n",
    "    if n < 2:\n",
    "        return n\n",
    "    \n",
    "    f_2 = 0\n",
    "    f_1 = 1\n",
    "    \n",
    "    for i in range(2, n+1):\n",
    "        f_0 = f_2 + f_1\n",
    "        f_2 = f_1\n",
    "        f_1 = f_0\n",
    "   \n",
    "    return f_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.6 µs ± 273 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "3.08 µs ± 87.4 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "fib_nomemory(10000)\n",
    "%timeit fib_iterative(10000)\n",
    "%timeit fib_nomemory(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# All done\n",
    "1. Compiled is quick, the interpreter inches\n",
    "2. Memory matters\n",
    "3. Big O is a big deal\n",
    "4. Favor fast functions\n",
    "5. Savor sparsity and structure\n",
    "6. Don't duplicate drudgery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Two closing thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A. Don't optimize prematurely!\n",
    "- When you first write code, write it as simply and cleanly and intelligibly as you can\n",
    "    - Maybe this code won't work on the real problem you have in mind, but it will probably work on a smaller version of the problem (less data, grids with fewer points, processes with fewer exogenous states, etc.)\n",
    "\n",
    "\n",
    "- You can use this to explore an idea, and get test benchmarks where we're pretty sure we have the right answer (because the code is so transparent)\n",
    "\n",
    "\n",
    "- *Then* as you profile and learn about bottlenecks, start to bring out all the tricks and get speedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# B. Computational power isn't limited to your laptop\n",
    "- For this lecture I'm running code on my personal machine, since it's the very simplest to set up, and with good coding practices we can do even quite complex problems\n",
    "\n",
    "\n",
    "- These 10x or 100x or 1000x speedups are often useful no matter what setup you're using!\n",
    "\n",
    "\n",
    "- But... cloud computing has made it a lot more convenient to call upon computing power as a commodity, so if there's some bottleneck, we can complement the tricks here by throwing cheap resources at the problem too\n",
    "    * Also makes it easier to use GPUs, etc."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
